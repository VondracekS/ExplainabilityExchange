{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "<center><img width=\"400\" height=\"100\" src=\"./imgs/ais.png\" class=\"imagedim\">\n",
    "</p>\n",
    "\n",
    "# Explainer Dashoards Demo Notebook\n",
    "*[Stepan Vondracek](https://people.telekom.de/businesscard?wr=200083284)*\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a demo notebook meant to accompany a presentation given within the AIS' Knowledge exchange.\n",
    "The capabilities of the explainer dashboards will be demonstrated using the\n",
    "[Titanic dataset](https://www.kaggle.com/c/titanic) aka the Wonderwall of data science. However, I believe that given\n",
    "the more general audience, the AI/ML experts will pardon me (and the public will appreciate the understandability\n",
    "of the data set.\n",
    "\n",
    "<p>\n",
    "<center><img width=\"300\" height=\"300\" src=\"./imgs/wonder.png\" class=\"imagedim\">\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Intro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run if the requirements are not satisfied\n",
    "# !pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I use the kaggle titanic data set. The test data does not contain the actual outcome, hence I will just split the train data set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"./data/titanic_train.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Create the new train/test split as kaggle test set does not contain target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = {'train': (train_test_split(titanic_data, test_size=0.2))[0],\n",
    "        'test': (train_test_split(titanic_data, test_size=0.2))[1]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook works just for the showcase of Explainer Dashboards, hence I will not perform any extensive\n",
    "feature engineering. I will just convert the *sex* and *passenger class* variables to dummies and drop the\n",
    "nominal variables and then drop all rows affected by missing observations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for name, tbl in data.items():\n",
    "    data[name] = (pd.get_dummies(tbl, columns=['Sex', 'Pclass'], drop_first=True)\n",
    "                 .drop(['Ticket', 'Cabin', 'Embarked', 'Name'], axis=1)\n",
    "                 .set_index('PassengerId')).dropna()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purposes of this showcase, I will use only two models. The first is just a GLM using the logit link function, the second (to demonstrate\n",
    "the capabilities of SHAP with more complex models) will be Random Forrest which I have previously tuned for\n",
    "the particular specification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = data['train'].drop('Survived', axis=1)\n",
    "y = data['train']['Survived']\n",
    "\n",
    "models_dict = {\n",
    "    'logit': LogisticRegression(fit_intercept=False),\n",
    "    'random_forrest': RandomForestClassifier(criterion='gini',\n",
    "                                             n_estimators=700,\n",
    "                                             min_samples_split=10,\n",
    "                                             min_samples_leaf=3,\n",
    "                                             max_features='auto',\n",
    "                                             oob_score=True,\n",
    "                                             random_state=1,\n",
    "                                             n_jobs=-1)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cells will just create, fit and predict using the particular specifications."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "%%capture\n",
    "#  Add features as an attribute, so I can easily later easily use them as an argument\n",
    "models_fit = {}\n",
    "for mod, specs in models_dict.items():\n",
    "    models_fit[mod] = specs.fit(X, y)\n",
    "    specs.features = list(X.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Get predictions on the test data\n",
    "models_pred = {}\n",
    "for mod, fit in models_fit.items():\n",
    "    models_pred[mod] = fit.predict(data['test'][fit.features])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logit has a MAE value of: 0.22\n",
      "Model random_forrest has a MAE value of: 0.09\n"
     ]
    }
   ],
   "source": [
    "# Get MAE of both models\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "models_mae = {}\n",
    "for mod, pred in models_pred.items():\n",
    "    models_mae[mod] = mae(pred, data['test']['Survived'])\n",
    "\n",
    "for k, v in models_mae.items():\n",
    "    print(f\"Model {k} has a MAE value of: {v:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  2 Explainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it's time to demonstrate the capabilities of  Explainer Dashboards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from explainerdashboard.explainers import ClassifierExplainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "%%capture\n",
    "explainers = {}\n",
    "\n",
    "for model, specs in models_dict.items():\n",
    "    explainers[model] = ClassifierExplainer(model=specs,\n",
    "                                            X=data['test'][models_fit[mod].features],\n",
    "                                            y=data['test']['Survived'],\n",
    "                                            model_output='probability',\n",
    "                                            index_name=\"Passenger ID\"\n",
    "                                            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ExplainerDashboard..\n",
      "Detected notebook environment, consider setting mode='external', mode='inline' or mode='jupyterlab' to keep the notebook interactive while the dashboard is running...\n",
      "Warning: calculating shap interaction values can be slow! Pass shap_interaction=False to remove interactions tab.\n",
      "Generating layout...\n",
      "Calculating shap values...\n",
      "Calculating prediction probabilities...\n",
      "Calculating metrics...\n",
      "Calculating confusion matrices...\n",
      "Calculating classification_dfs...\n",
      "Calculating roc auc curves...\n",
      "Calculating pr auc curves...\n",
      "Calculating liftcurve_dfs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating shap interaction values... (this may take a while)\n",
      "Reminder: TreeShap computational complexity is O(TLD^2), where T is the number of trees, L is the maximum number of leaves in any tree and D the maximal depth of any tree. So reducing these will speed up the calculation.\n",
      "Calculating dependencies...\n",
      "Calculating permutation importances (if slow, try setting n_jobs parameter)...\n",
      "Calculating predictions...\n",
      "Calculating pred_percentiles...\n",
      "Calculating ShadowDecTree for each individual decision tree...\n",
      "Reminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')\n",
      "Registering callbacks...\n",
      "Warning: Original ExplainerDashboard was initialized with mode='dash'. Rebuilding dashboard before launch:\n",
      "Building ExplainerDashboard..\n",
      "Warning: calculating shap interaction values can be slow! Pass shap_interaction=False to remove interactions tab.\n",
      "Generating layout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n",
      "C:\\Users\\A200083283\\OneDrive - Deutsche Telekom AG\\non_DT\\uni\\WS_22\\venv\\lib\\site-packages\\explainerdashboard\\explainer_methods.py:791: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating dependencies...\n",
      "Reminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')\n",
      "Registering callbacks...\n",
      "Starting ExplainerDashboard on http://10.180.109.80:8050\n",
      "You can terminate the dashboard with ExplainerDashboard.terminate(8050)\n"
     ]
    }
   ],
   "source": [
    "from explainerdashboard import ExplainerDashboard\n",
    "ExplainerDashboard(explainers['random_forrest']).run(mode='external')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Conclusion\n",
    "\n",
    "This very simple demo notebook was meant to briefly demonstrate the easy-to-use, yet capable Python library. Feel free\n",
    "to contact me as I will surely explore their capabilities more in depth."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
